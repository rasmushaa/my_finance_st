{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome My Utils","text":"<p>This is some markdown info.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>backend<ul> <li>categories<ul> <li>categories</li> </ul> </li> <li>credentials<ul> <li>authentication</li> <li>user</li> </ul> </li> <li>filesystem<ul> <li>data_collector</li> <li>database</li> <li>parser</li> </ul> </li> <li>google_cloud<ul> <li>api</li> </ul> </li> <li>ml<ul> <li>ml</li> <li>model</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/backend/categories/categories/","title":"categories","text":""},{"location":"reference/backend/credentials/authentication/","title":"authentication","text":""},{"location":"reference/backend/credentials/user/","title":"user","text":""},{"location":"reference/backend/filesystem/data_collector/","title":"data_collector","text":""},{"location":"reference/backend/filesystem/data_collector/#backend.filesystem.data_collector.DataCollector","title":"<code>DataCollector</code>","text":"Source code in <code>src/backend/filesystem/data_collector.py</code> <pre><code>class DataCollector:\n    def __init__(self, **kwargs):\n        \"\"\" Initialize the class with user-defined member variables.\n        \"\"\"\n        self.__dict__.update(kwargs)\n\n    def add_from_tuple(self, items):\n        \"\"\" Add member variables from a list of tuples \n        where each tuple contains a name and value.\n\n        Inputs\n        ------\n        items : list of tuples\n            [(key, value), (key2, value2), ...]\n        \"\"\"\n        for name, value in items:\n            self.__dict__[name] = value\n\n    def add_from_list(self, key_list):\n        \"\"\" Add member variables from a list of where each value is a new key, \n        and default values are Nones.\n\n        Inputs\n        ------\n        key_list : list\n            [key1, key2, ...]\n        \"\"\"\n        for key in key_list:\n            self.__dict__[key] = None\n\n    def no_nones(self) -&gt; bool:\n        \"\"\" Validate that user has updated all \n        possible data entrys\n        \"\"\"\n        for key, value in self.__dict__.items():\n            if value is None:\n                return False\n        return True\n\n\n    def to_dataframe(self):\n        \"\"\" Convert member variables into a pandas DataFrame where\n        each row contains the variable name and its value.\n        \"\"\"\n        data = {\n            'key': list(self.__dict__.keys()),\n            'value': list(self.__dict__.values())\n        }\n        return pd.DataFrame(data)\n\n    def __str__(self):\n        string = 'DataCollector:\\n'\n        for key, value in self.__dict__.items():\n            string += f'Key: {key}, Value: {value}\\n'\n        return string\n</code></pre>"},{"location":"reference/backend/filesystem/data_collector/#backend.filesystem.data_collector.DataCollector.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the class with user-defined member variables.</p> Source code in <code>src/backend/filesystem/data_collector.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\" Initialize the class with user-defined member variables.\n    \"\"\"\n    self.__dict__.update(kwargs)\n</code></pre>"},{"location":"reference/backend/filesystem/data_collector/#backend.filesystem.data_collector.DataCollector.add_from_list","title":"<code>add_from_list(key_list)</code>","text":"<p>Add member variables from a list of where each value is a new key,  and default values are Nones.</p> Inputs <p>key_list : list     [key1, key2, ...]</p> Source code in <code>src/backend/filesystem/data_collector.py</code> <pre><code>def add_from_list(self, key_list):\n    \"\"\" Add member variables from a list of where each value is a new key, \n    and default values are Nones.\n\n    Inputs\n    ------\n    key_list : list\n        [key1, key2, ...]\n    \"\"\"\n    for key in key_list:\n        self.__dict__[key] = None\n</code></pre>"},{"location":"reference/backend/filesystem/data_collector/#backend.filesystem.data_collector.DataCollector.add_from_tuple","title":"<code>add_from_tuple(items)</code>","text":"<p>Add member variables from a list of tuples  where each tuple contains a name and value.</p> Inputs <p>items : list of tuples     [(key, value), (key2, value2), ...]</p> Source code in <code>src/backend/filesystem/data_collector.py</code> <pre><code>def add_from_tuple(self, items):\n    \"\"\" Add member variables from a list of tuples \n    where each tuple contains a name and value.\n\n    Inputs\n    ------\n    items : list of tuples\n        [(key, value), (key2, value2), ...]\n    \"\"\"\n    for name, value in items:\n        self.__dict__[name] = value\n</code></pre>"},{"location":"reference/backend/filesystem/data_collector/#backend.filesystem.data_collector.DataCollector.no_nones","title":"<code>no_nones()</code>","text":"<p>Validate that user has updated all  possible data entrys</p> Source code in <code>src/backend/filesystem/data_collector.py</code> <pre><code>def no_nones(self) -&gt; bool:\n    \"\"\" Validate that user has updated all \n    possible data entrys\n    \"\"\"\n    for key, value in self.__dict__.items():\n        if value is None:\n            return False\n    return True\n</code></pre>"},{"location":"reference/backend/filesystem/data_collector/#backend.filesystem.data_collector.DataCollector.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert member variables into a pandas DataFrame where each row contains the variable name and its value.</p> Source code in <code>src/backend/filesystem/data_collector.py</code> <pre><code>def to_dataframe(self):\n    \"\"\" Convert member variables into a pandas DataFrame where\n    each row contains the variable name and its value.\n    \"\"\"\n    data = {\n        'key': list(self.__dict__.keys()),\n        'value': list(self.__dict__.values())\n    }\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/backend/filesystem/database/","title":"database","text":""},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database","title":"<code>Database</code>","text":"Source code in <code>src/backend/filesystem/database.py</code> <pre><code>class Database():\n    def __init__(self):\n        self.__client = GoogleCloudAPI()\n\n\n    def add_transactions_to_database(self, df: pd.DataFrame, user_name: str) -&gt; bool:\n        ''' Push one Banking file to database.\n\n        The table is created, if not already exist.\n        Ether the whoele df is uploaded, or it fails completely.\n\n        Inputs\n        ------\n        df: pd.DataFrame\n            The Banking File\n        user_name: str\n            The current active user\n        '''\n        df['Category'] = df['Category'].fillna('N/A') # Different missing values can have multiple values: Nan, Empty, etc. Thus, 'N/A' is selected to handle this\n        df['Receiver'] = df['Receiver'].fillna('missing') # There may be none fields in the data, but BigQuery does not allow those\n        df['KeyUser'] = user_name\n        df['CommitTimestamp'] = pd.Timestamp('now', tz='Europe/Helsinki')\n        df = df[['KeyDate', 'KeyUser', 'Amount', 'Receiver', 'Category', 'CommitTimestamp']]\n        try:\n            self.__client.write_pandas_to_table(df, 'f_transactions')\n        except:\n            return False\n        return True\n\n\n    def add_filetype_to_databases(self, collector) -&gt; bool:\n        ''' Add a new supported filetype row to database.\n\n        The ColumnNameString is a common string that contains all of the column names,\n        that is used to identify different file.\n        The table contains required information to select correct columns to run analysis.\n\n        Inputs\n        ------\n        collector\n            DataCollector object with filled values from GUI\n        '''\n        if not collector.no_nones(): # User must have inputted something to all possible entries\n            return False\n\n        df_collection = collector.to_dataframe()\n\n        df_collection['my_index'] = 0\n        df_collection = df_collection.pivot(index='my_index', columns='key', values='value').reset_index(drop=True)\n        df_collection = df_collection[['KeyFileName', 'DateColumn', 'DateColumnFormat', 'AmountColumn', 'ReceiverColumn', 'ColumnNameString']]\n\n        try:\n            self.__client.write_pandas_to_table(df_collection, 'd_filetypes')\n        except:\n            return False\n        return True\n\n\n    def add_assets_to_database(self, date, user_name, collector) -&gt; bool:\n        ''' Insert new Quarter for a user\n\n        Inputs\n        ------\n        date : Date object\n            The insertation date selected by a user. The qurter level is used on the reporting level\n        user_name : str\n            User name key for table aggregation\n        collector\n            DataCollector object with filled values from GUI\n\n        Returns\n        -------\n        Inseration success\n        '''\n        if not collector.no_nones(): # User must have inputted something to all possible entries\n            return False\n\n        df_collection = collector.to_dataframe()\n\n        df = pd.DataFrame(columns=['KeyDate', 'KeyUser', 'Category', 'Value'])\n        df['Category'] = df_collection['key'].str.upper().str.replace('_', '-') # Keys back to BigQuery format\n        df['Value'] = df_collection['value']\n        df['KeyDate'] = date\n        df['KeyUser'] = user_name\n        df['CommitTimestamp'] = pd.Timestamp('now', tz='Europe/Helsinki')\n\n        try:\n            self.__client.write_pandas_to_table(df, 'f_assets')\n        except:\n            return False\n        return True\n\n\n    def get_asset_data_collector(self):\n        ''' Init a class that has possble data entrys\n        as member variables.\n\n        Returns\n        -------\n        DataCollector() with Asset-categories\n        '''\n        assets = Categories().get_asset_categories()\n        assets = [asset.replace('-', '_').lower() for asset in assets] # ASSETS-ARE-IN-THIS-FORMAT, and must_formatted_for_python\n        collector = DataCollector()\n        collector.add_from_list(assets)\n        return collector\n\n\n    def get_filetype_data_collector(self):\n        ''' Init a class that has possble data entrys\n        as member variables.\n\n        Returns\n        -------\n        DataCollector() with FileType-categories\n        '''  \n        variables = [\n            'KeyFileName', \n            'DateColumn', \n            'DateColumnFormat', \n            'AmountColumn', \n            'ReceiverColumn', \n            'ColumnNameString'\n            ]\n        collector = DataCollector()\n        collector.add_from_list(variables)\n        return collector\n\n\n    def date_not_in_transactions_table(self, date, user_name: str):\n        ''' Prevent from adding multiple same banking files to the database.\n        The date validation is user specific.\n\n        Inputs\n        ------\n        date: datetime\n            The minimum date in the Banking File\n        user_name: str\n            The current active user\n        '''\n        sql = f'''\n        SELECT\n            MAX(KeyDate) AS date\n        FROM\n            {self.__client._dataset}.f_transactions\n        WHERE\n            KeyUser = '{user_name}'\n        '''\n        latest_date = self.__client.sql_to_pandas(sql)['date'][0]\n        latest_date = pd.to_datetime(latest_date, format='%Y-%m-%d').date()\n        return (latest_date is None or pd.isnull(latest_date)) or (date &gt; pd.to_datetime(latest_date, format='%Y-%m-%d').date()) # If there is data, validate\n\n\n    def date_not_in_assets_table(self, date, user_name: str):\n        ''' Prevent from adding multiple same assets files to the database.\n        The date validation is user specific.\n\n        Inputs\n        ------\n        date: datetime\n            The asset File date\n        user_name: str\n            The current active user\n        '''\n        sql = f'''\n        SELECT\n            MAX(KeyDate) AS date\n        FROM\n            {self.__client._dataset}.f_assets\n        WHERE\n            KeyUser = '{user_name}'\n        '''\n        latest_date = self.__client.sql_to_pandas(sql)['date'][0]\n        latest_date = pd.to_datetime(latest_date, format='%Y-%m-%d').date()\n        return (latest_date is None or pd.isnull(latest_date)) or (date &gt; pd.to_datetime(latest_date, format='%Y-%m-%d').date()) # If there is data, validate\n\n\n    def filetype_is_in_database(self, df: pd.DataFrame) -&gt; bool:\n        ''' Checks whether the file type is known\n\n        Inputs\n        ------\n        df: pd.DataFrame\n            The user input csv file \n        '''\n        cols = df.columns.to_list()\n        col_str = ','.join(cols)\n        sql = f\"\"\"\n        SELECT\n            COUNT(*) AS count\n        FROM\n            `{self.__client._dataset}.d_filetypes`\n        WHERE\n            ColumnNameString = '{col_str}'\n        \"\"\"\n        df = self.__client.sql_to_pandas(sql)\n        return df['count'].all() &gt; 0\n\n\n    def get_all_push_insertions(self) -&gt; pd.DataFrame:\n        ''' Get summary of all insertions done by users.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the summary of all insertions on user, and timestamp level.\n        '''\n        def query(table: str):\n            sql = f'''\n            SELECT\n                KeyUser,\n                \"{table}\" as TableName,\n                CommitTimestamp,\n                COUNT(*) AS RowCount,\n                MIN(KeyDate) AS MinDate,\n                MAX(KeyDate) AS MaxDate\n            FROM\n                {self.__client._dataset}.{table}\n            GROUP BY\n                KeyUser,\n                TableName,\n                CommitTimestamp\n            '''\n            return sql\n\n        df_transactions = self.__client.sql_to_pandas(query('f_transactions'))\n        df_assets = self.__client.sql_to_pandas(query('f_assets'))\n        df = pd.concat([df_transactions, df_assets], ignore_index=True)\n        df['CommitTimestamp'] = pd.to_datetime(df['CommitTimestamp'], errors='coerce', utc=True)\n        df = df.sort_values(by=['KeyUser', 'TableName', 'CommitTimestamp'], ascending=False).reset_index(drop=True)\n        return df\n\n\n    def delete_push_insertion(self, user_name: str, commit_timestamp: pd.Timestamp, table: str) -&gt; None:\n        ''' Delete a specific insertation batch from database.\n\n        Inputs\n        ------\n        user_name: str\n            The user who made the insertation\n        commit_timestamp: pd.Timestamp\n            The exact timestamp when the insertation was made\n        table: str\n            The table name where the insertation is deleted from\n        '''\n        sql = f'''\n        DELETE FROM\n            {self.__client._dataset}.{table}\n        WHERE\n            KeyUser = '{user_name}'\n            AND CommitTimestamp = '{commit_timestamp}'\n        '''\n        self.__client.sql_query(sql)\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.add_assets_to_database","title":"<code>add_assets_to_database(date, user_name, collector)</code>","text":"<p>Insert new Quarter for a user</p> Inputs <p>date : Date object     The insertation date selected by a user. The qurter level is used on the reporting level user_name : str     User name key for table aggregation collector     DataCollector object with filled values from GUI</p> <p>Returns:</p> Type Description <code>Inseration success</code> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def add_assets_to_database(self, date, user_name, collector) -&gt; bool:\n    ''' Insert new Quarter for a user\n\n    Inputs\n    ------\n    date : Date object\n        The insertation date selected by a user. The qurter level is used on the reporting level\n    user_name : str\n        User name key for table aggregation\n    collector\n        DataCollector object with filled values from GUI\n\n    Returns\n    -------\n    Inseration success\n    '''\n    if not collector.no_nones(): # User must have inputted something to all possible entries\n        return False\n\n    df_collection = collector.to_dataframe()\n\n    df = pd.DataFrame(columns=['KeyDate', 'KeyUser', 'Category', 'Value'])\n    df['Category'] = df_collection['key'].str.upper().str.replace('_', '-') # Keys back to BigQuery format\n    df['Value'] = df_collection['value']\n    df['KeyDate'] = date\n    df['KeyUser'] = user_name\n    df['CommitTimestamp'] = pd.Timestamp('now', tz='Europe/Helsinki')\n\n    try:\n        self.__client.write_pandas_to_table(df, 'f_assets')\n    except:\n        return False\n    return True\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.add_filetype_to_databases","title":"<code>add_filetype_to_databases(collector)</code>","text":"<p>Add a new supported filetype row to database.</p> <p>The ColumnNameString is a common string that contains all of the column names, that is used to identify different file. The table contains required information to select correct columns to run analysis.</p> Inputs <p>collector     DataCollector object with filled values from GUI</p> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def add_filetype_to_databases(self, collector) -&gt; bool:\n    ''' Add a new supported filetype row to database.\n\n    The ColumnNameString is a common string that contains all of the column names,\n    that is used to identify different file.\n    The table contains required information to select correct columns to run analysis.\n\n    Inputs\n    ------\n    collector\n        DataCollector object with filled values from GUI\n    '''\n    if not collector.no_nones(): # User must have inputted something to all possible entries\n        return False\n\n    df_collection = collector.to_dataframe()\n\n    df_collection['my_index'] = 0\n    df_collection = df_collection.pivot(index='my_index', columns='key', values='value').reset_index(drop=True)\n    df_collection = df_collection[['KeyFileName', 'DateColumn', 'DateColumnFormat', 'AmountColumn', 'ReceiverColumn', 'ColumnNameString']]\n\n    try:\n        self.__client.write_pandas_to_table(df_collection, 'd_filetypes')\n    except:\n        return False\n    return True\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.add_transactions_to_database","title":"<code>add_transactions_to_database(df, user_name)</code>","text":"<p>Push one Banking file to database.</p> <p>The table is created, if not already exist. Ether the whoele df is uploaded, or it fails completely.</p> Inputs <p>df: pd.DataFrame     The Banking File user_name: str     The current active user</p> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def add_transactions_to_database(self, df: pd.DataFrame, user_name: str) -&gt; bool:\n    ''' Push one Banking file to database.\n\n    The table is created, if not already exist.\n    Ether the whoele df is uploaded, or it fails completely.\n\n    Inputs\n    ------\n    df: pd.DataFrame\n        The Banking File\n    user_name: str\n        The current active user\n    '''\n    df['Category'] = df['Category'].fillna('N/A') # Different missing values can have multiple values: Nan, Empty, etc. Thus, 'N/A' is selected to handle this\n    df['Receiver'] = df['Receiver'].fillna('missing') # There may be none fields in the data, but BigQuery does not allow those\n    df['KeyUser'] = user_name\n    df['CommitTimestamp'] = pd.Timestamp('now', tz='Europe/Helsinki')\n    df = df[['KeyDate', 'KeyUser', 'Amount', 'Receiver', 'Category', 'CommitTimestamp']]\n    try:\n        self.__client.write_pandas_to_table(df, 'f_transactions')\n    except:\n        return False\n    return True\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.date_not_in_assets_table","title":"<code>date_not_in_assets_table(date, user_name)</code>","text":"<p>Prevent from adding multiple same assets files to the database. The date validation is user specific.</p> Inputs <p>date: datetime     The asset File date user_name: str     The current active user</p> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def date_not_in_assets_table(self, date, user_name: str):\n    ''' Prevent from adding multiple same assets files to the database.\n    The date validation is user specific.\n\n    Inputs\n    ------\n    date: datetime\n        The asset File date\n    user_name: str\n        The current active user\n    '''\n    sql = f'''\n    SELECT\n        MAX(KeyDate) AS date\n    FROM\n        {self.__client._dataset}.f_assets\n    WHERE\n        KeyUser = '{user_name}'\n    '''\n    latest_date = self.__client.sql_to_pandas(sql)['date'][0]\n    latest_date = pd.to_datetime(latest_date, format='%Y-%m-%d').date()\n    return (latest_date is None or pd.isnull(latest_date)) or (date &gt; pd.to_datetime(latest_date, format='%Y-%m-%d').date()) # If there is data, validate\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.date_not_in_transactions_table","title":"<code>date_not_in_transactions_table(date, user_name)</code>","text":"<p>Prevent from adding multiple same banking files to the database. The date validation is user specific.</p> Inputs <p>date: datetime     The minimum date in the Banking File user_name: str     The current active user</p> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def date_not_in_transactions_table(self, date, user_name: str):\n    ''' Prevent from adding multiple same banking files to the database.\n    The date validation is user specific.\n\n    Inputs\n    ------\n    date: datetime\n        The minimum date in the Banking File\n    user_name: str\n        The current active user\n    '''\n    sql = f'''\n    SELECT\n        MAX(KeyDate) AS date\n    FROM\n        {self.__client._dataset}.f_transactions\n    WHERE\n        KeyUser = '{user_name}'\n    '''\n    latest_date = self.__client.sql_to_pandas(sql)['date'][0]\n    latest_date = pd.to_datetime(latest_date, format='%Y-%m-%d').date()\n    return (latest_date is None or pd.isnull(latest_date)) or (date &gt; pd.to_datetime(latest_date, format='%Y-%m-%d').date()) # If there is data, validate\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.delete_push_insertion","title":"<code>delete_push_insertion(user_name, commit_timestamp, table)</code>","text":"<p>Delete a specific insertation batch from database.</p> Inputs <p>user_name: str     The user who made the insertation commit_timestamp: pd.Timestamp     The exact timestamp when the insertation was made table: str     The table name where the insertation is deleted from</p> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def delete_push_insertion(self, user_name: str, commit_timestamp: pd.Timestamp, table: str) -&gt; None:\n    ''' Delete a specific insertation batch from database.\n\n    Inputs\n    ------\n    user_name: str\n        The user who made the insertation\n    commit_timestamp: pd.Timestamp\n        The exact timestamp when the insertation was made\n    table: str\n        The table name where the insertation is deleted from\n    '''\n    sql = f'''\n    DELETE FROM\n        {self.__client._dataset}.{table}\n    WHERE\n        KeyUser = '{user_name}'\n        AND CommitTimestamp = '{commit_timestamp}'\n    '''\n    self.__client.sql_query(sql)\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.filetype_is_in_database","title":"<code>filetype_is_in_database(df)</code>","text":"<p>Checks whether the file type is known</p> Inputs <p>df: pd.DataFrame     The user input csv file</p> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def filetype_is_in_database(self, df: pd.DataFrame) -&gt; bool:\n    ''' Checks whether the file type is known\n\n    Inputs\n    ------\n    df: pd.DataFrame\n        The user input csv file \n    '''\n    cols = df.columns.to_list()\n    col_str = ','.join(cols)\n    sql = f\"\"\"\n    SELECT\n        COUNT(*) AS count\n    FROM\n        `{self.__client._dataset}.d_filetypes`\n    WHERE\n        ColumnNameString = '{col_str}'\n    \"\"\"\n    df = self.__client.sql_to_pandas(sql)\n    return df['count'].all() &gt; 0\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.get_all_push_insertions","title":"<code>get_all_push_insertions()</code>","text":"<p>Get summary of all insertions done by users.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the summary of all insertions on user, and timestamp level.</p> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def get_all_push_insertions(self) -&gt; pd.DataFrame:\n    ''' Get summary of all insertions done by users.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the summary of all insertions on user, and timestamp level.\n    '''\n    def query(table: str):\n        sql = f'''\n        SELECT\n            KeyUser,\n            \"{table}\" as TableName,\n            CommitTimestamp,\n            COUNT(*) AS RowCount,\n            MIN(KeyDate) AS MinDate,\n            MAX(KeyDate) AS MaxDate\n        FROM\n            {self.__client._dataset}.{table}\n        GROUP BY\n            KeyUser,\n            TableName,\n            CommitTimestamp\n        '''\n        return sql\n\n    df_transactions = self.__client.sql_to_pandas(query('f_transactions'))\n    df_assets = self.__client.sql_to_pandas(query('f_assets'))\n    df = pd.concat([df_transactions, df_assets], ignore_index=True)\n    df['CommitTimestamp'] = pd.to_datetime(df['CommitTimestamp'], errors='coerce', utc=True)\n    df = df.sort_values(by=['KeyUser', 'TableName', 'CommitTimestamp'], ascending=False).reset_index(drop=True)\n    return df\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.get_asset_data_collector","title":"<code>get_asset_data_collector()</code>","text":"<p>Init a class that has possble data entrys as member variables.</p> <p>Returns:</p> Type Description <code>DataCollector() with Asset-categories</code> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def get_asset_data_collector(self):\n    ''' Init a class that has possble data entrys\n    as member variables.\n\n    Returns\n    -------\n    DataCollector() with Asset-categories\n    '''\n    assets = Categories().get_asset_categories()\n    assets = [asset.replace('-', '_').lower() for asset in assets] # ASSETS-ARE-IN-THIS-FORMAT, and must_formatted_for_python\n    collector = DataCollector()\n    collector.add_from_list(assets)\n    return collector\n</code></pre>"},{"location":"reference/backend/filesystem/database/#backend.filesystem.database.Database.get_filetype_data_collector","title":"<code>get_filetype_data_collector()</code>","text":"<p>Init a class that has possble data entrys as member variables.</p> <p>Returns:</p> Type Description <code>DataCollector() with FileType-categories</code> Source code in <code>src/backend/filesystem/database.py</code> <pre><code>def get_filetype_data_collector(self):\n    ''' Init a class that has possble data entrys\n    as member variables.\n\n    Returns\n    -------\n    DataCollector() with FileType-categories\n    '''  \n    variables = [\n        'KeyFileName', \n        'DateColumn', \n        'DateColumnFormat', \n        'AmountColumn', \n        'ReceiverColumn', \n        'ColumnNameString'\n        ]\n    collector = DataCollector()\n    collector.add_from_list(variables)\n    return collector\n</code></pre>"},{"location":"reference/backend/filesystem/parser/","title":"parser","text":""},{"location":"reference/backend/filesystem/parser/#backend.filesystem.parser.FileParser","title":"<code>FileParser</code>","text":"Source code in <code>src/backend/filesystem/parser.py</code> <pre><code>class FileParser:\n    def __init__(self):\n        self.__client = GoogleCloudAPI()\n\n    def open_binary_as_pandas(self, input_file) -&gt; pd.DataFrame:\n        ''' Open provided unkown CSV safely.\n\n        The text encoding, and seperator characters are unkown,\n        and must be determined manaully using external libraries.\n\n        Inputs\n        ------\n        input_file : Streamlit BytesIO\n            User provided file, that is validate to be a csv\n        '''\n        encoding, separator = self.__autodetect_file_coding(input_file)\n        df = pd.read_csv(input_file, encoding=encoding, sep=separator)\n        return df\n\n\n    def transform_input_file(self, df: pd.DataFrame):\n        ''' The Raw CSV input file is transformed into the required format\n\n        The file is assumed to be known in this part, and its recorded column\n        format is quered to transform it into the expected format.\n        Floats and Dates are also handled.\n\n        Inputs\n        ------\n        df: pd.DataFrame\n            The user input csv file \n        '''\n        cols = df.columns.to_list()\n        col_str = ','.join(cols)\n        sql=f\"\"\"\n        SELECT\n            *\n        FROM\n            `{self.__client._dataset}.d_filetypes`\n        WHERE\n            ColumnNameString = '{col_str}'\n        \"\"\"\n        filetype = self.__client.sql_to_pandas(sql).iloc[0].to_dict()\n\n        df.rename(columns={filetype['DateColumn']: 'KeyDate', filetype['ReceiverColumn']: 'Receiver', filetype['AmountColumn']: 'Amount'}, inplace=True)\n\n        df['KeyDate'] = pd.to_datetime(df['KeyDate'], format=filetype['DateColumnFormat']).dt.date\n        if df['Amount'].dtype == 'object':\n            df['Amount'] = df['Amount'].str.replace(',', '.').astype(float) if df['Amount'].str.contains(',').any() else df['Amount'].astype(float) # Decimal ',' to '.' float\n        df['Category'] = None\n\n        df = df[['KeyDate', 'Amount', 'Receiver', 'Category']].copy()\n        df.sort_values(by='KeyDate', ascending=True, inplace=True)\n        return df\n\n\n    def __autodetect_file_coding(self, file_binary) -&gt; str:\n        ''' \n        Auto detects used encoding and separator in csv file.\n\n        If file parameters are unkwown, it has to be first opened in binary\n        to avoid any parsing errors.\n\n        Parameters\n        ----------\n        file_binary : A subclass of BytesIO\n            The raw input file from Streamlit File Uploader\n\n        Returns\n        -------\n        encoding : str\n            Detected encoding. Note, chardet works well, but its not perfect!\n        separator : str\n            Detected separator in [',', ';', '', '\\t', '|']\n        '''\n        encoding_dict = chardet.detect(file_binary.getvalue())\n        encoding = encoding_dict['encoding']\n\n        dialect = csv.Sniffer().sniff(file_binary.getvalue().decode(encoding), delimiters=[',', ';', '', '\\t', '|'])\n        separator = dialect.delimiter\n\n        return encoding, separator\n</code></pre>"},{"location":"reference/backend/filesystem/parser/#backend.filesystem.parser.FileParser.__autodetect_file_coding","title":"<code>__autodetect_file_coding(file_binary)</code>","text":"<p>Auto detects used encoding and separator in csv file.</p> <p>If file parameters are unkwown, it has to be first opened in binary to avoid any parsing errors.</p> <p>Parameters:</p> Name Type Description Default <code>file_binary</code> <code>A subclass of BytesIO</code> <p>The raw input file from Streamlit File Uploader</p> required <p>Returns:</p> Name Type Description <code>encoding</code> <code>str</code> <p>Detected encoding. Note, chardet works well, but its not perfect!</p> <code>separator</code> <code>str</code> <p>Detected separator in [',', ';', '', '      ', '|']</p> Source code in <code>src/backend/filesystem/parser.py</code> <pre><code>def __autodetect_file_coding(self, file_binary) -&gt; str:\n    ''' \n    Auto detects used encoding and separator in csv file.\n\n    If file parameters are unkwown, it has to be first opened in binary\n    to avoid any parsing errors.\n\n    Parameters\n    ----------\n    file_binary : A subclass of BytesIO\n        The raw input file from Streamlit File Uploader\n\n    Returns\n    -------\n    encoding : str\n        Detected encoding. Note, chardet works well, but its not perfect!\n    separator : str\n        Detected separator in [',', ';', '', '\\t', '|']\n    '''\n    encoding_dict = chardet.detect(file_binary.getvalue())\n    encoding = encoding_dict['encoding']\n\n    dialect = csv.Sniffer().sniff(file_binary.getvalue().decode(encoding), delimiters=[',', ';', '', '\\t', '|'])\n    separator = dialect.delimiter\n\n    return encoding, separator\n</code></pre>"},{"location":"reference/backend/filesystem/parser/#backend.filesystem.parser.FileParser.open_binary_as_pandas","title":"<code>open_binary_as_pandas(input_file)</code>","text":"<p>Open provided unkown CSV safely.</p> <p>The text encoding, and seperator characters are unkown, and must be determined manaully using external libraries.</p> Inputs <p>input_file : Streamlit BytesIO     User provided file, that is validate to be a csv</p> Source code in <code>src/backend/filesystem/parser.py</code> <pre><code>def open_binary_as_pandas(self, input_file) -&gt; pd.DataFrame:\n    ''' Open provided unkown CSV safely.\n\n    The text encoding, and seperator characters are unkown,\n    and must be determined manaully using external libraries.\n\n    Inputs\n    ------\n    input_file : Streamlit BytesIO\n        User provided file, that is validate to be a csv\n    '''\n    encoding, separator = self.__autodetect_file_coding(input_file)\n    df = pd.read_csv(input_file, encoding=encoding, sep=separator)\n    return df\n</code></pre>"},{"location":"reference/backend/filesystem/parser/#backend.filesystem.parser.FileParser.transform_input_file","title":"<code>transform_input_file(df)</code>","text":"<p>The Raw CSV input file is transformed into the required format</p> <p>The file is assumed to be known in this part, and its recorded column format is quered to transform it into the expected format. Floats and Dates are also handled.</p> Inputs <p>df: pd.DataFrame     The user input csv file</p> Source code in <code>src/backend/filesystem/parser.py</code> <pre><code>def transform_input_file(self, df: pd.DataFrame):\n    ''' The Raw CSV input file is transformed into the required format\n\n    The file is assumed to be known in this part, and its recorded column\n    format is quered to transform it into the expected format.\n    Floats and Dates are also handled.\n\n    Inputs\n    ------\n    df: pd.DataFrame\n        The user input csv file \n    '''\n    cols = df.columns.to_list()\n    col_str = ','.join(cols)\n    sql=f\"\"\"\n    SELECT\n        *\n    FROM\n        `{self.__client._dataset}.d_filetypes`\n    WHERE\n        ColumnNameString = '{col_str}'\n    \"\"\"\n    filetype = self.__client.sql_to_pandas(sql).iloc[0].to_dict()\n\n    df.rename(columns={filetype['DateColumn']: 'KeyDate', filetype['ReceiverColumn']: 'Receiver', filetype['AmountColumn']: 'Amount'}, inplace=True)\n\n    df['KeyDate'] = pd.to_datetime(df['KeyDate'], format=filetype['DateColumnFormat']).dt.date\n    if df['Amount'].dtype == 'object':\n        df['Amount'] = df['Amount'].str.replace(',', '.').astype(float) if df['Amount'].str.contains(',').any() else df['Amount'].astype(float) # Decimal ',' to '.' float\n    df['Category'] = None\n\n    df = df[['KeyDate', 'Amount', 'Receiver', 'Category']].copy()\n    df.sort_values(by='KeyDate', ascending=True, inplace=True)\n    return df\n</code></pre>"},{"location":"reference/backend/google_cloud/api/","title":"api","text":""},{"location":"reference/backend/google_cloud/api/#backend.google_cloud.api.GoogleCloudAPI","title":"<code>GoogleCloudAPI</code>","text":"Source code in <code>src/backend/google_cloud/api.py</code> <pre><code>class GoogleCloudAPI():\n    def __init__(self):\n        self.__project_id = os.getenv('GCP_PROJECT_ID')\n        self._dataset = os.getenv('GCP_BQ_DATASET') + '_' +  os.getenv('STREAMLIT_ENV')\n        self.__location = os.getenv('GCP_LOCATION')\n        self.__bucket_name = os.getenv('GCP_CGS_BUCKET')\n        self.__bucket_dir = os.getenv('GCP_CGS_BUCKET_DIR')\n\n\n    def sql_query(self, sql: str) -&gt; bigquery.table.RowIterator:\n        ''' Run a regular SQL query \n        and return a RowIterator.\n        '''\n        self.__debug(sql=sql)\n        client = bigquery.Client(location=self.__location, project=self.__project_id) # Use default Account/Cloud Run SA\n        query_job = client.query(sql)\n        return query_job.result()\n\n\n    def sql_to_pandas(self, sql: str) -&gt; pd.DataFrame:\n        ''' Run a regular SQL query \n        and return a pandas DataFrame.\n\n        Inputs\n        ------\n        sql : string\n            A regular SQL query\n\n        Returns\n        -------\n        df : DataFrame\n        '''\n        self.__debug(sql=sql)\n        df = pandas_gbq.read_gbq(sql, \n                                 project_id=self.__project_id,\n                                 location=self.__location, \n                                 progress_bar_type=None) # Use default Account/Cloud Run SA\n        return df\n\n\n    def write_pandas_to_table(self, df: pd.DataFrame, table: str):\n        ''' Push a DataFrame to BigQuery.\n\n        A new table will be create, if the destination does not exists,\n        however, pyarrows has a bug and it fails for datetime columns,\n        thus the schema must be constructed manually from pandas to GBQ format.\n        The mode is locked to Append only, to prevent accidental overwrites\n\n        Inputs\n        ------\n        df : pd.DataFram\n            A regular DataFrame\n        table : str\n            The name of destination Table, that is used together with initial project parameters\n        '''\n        table_schema = [] # [{'name': 'col1', 'type': 'STRING'},...]\n        for col in df.columns:\n            if 'date' in col.lower():\n                 table_schema.append({'name': col, 'type': 'DATE'})\n            elif 'object' in str(df[col].dtype):\n                 table_schema.append({'name': col, 'type': 'STRING'})\n            elif 'float' in str(df[col].dtype):\n                table_schema.append({'name': col, 'type': 'FLOAT64'})\n            elif 'datetime' in str(df[col].dtype):\n                table_schema.append({'name': col, 'type': 'TIMESTAMP'})\n\n        pandas_gbq.to_gbq(df, \n                          destination_table=f'{self._dataset}.{table}',\n                          project_id=self.__project_id, \n                          location=self.__location, \n                          table_schema=table_schema,\n                          if_exists='append',\n                          ) # Use default Account/Cloud Run SA\n\n\n    def upload_file_to_gcs(self, local_file_path: str):\n        ''' Upload Local File to GCS\n\n        The Bucker and folder are project specific,\n        and only the &lt;local_file_path&gt; is required\n\n        Inputs\n        ------\n        local_file_path : str\n            Name/Dir of the file to be uploaded with the same dir\n        '''\n        gcs_path = os.path.join(self.__bucket_dir, local_file_path)\n\n        client = storage.Client() # Use default Account/Cloud Run SA\n        bucket = client.get_bucket(self.__bucket_name)\n        blob = bucket.blob(gcs_path)\n        blob.upload_from_filename(local_file_path) \n\n\n    def download_file_from_gcs(self, local_file_path: str):\n        ''' Download a file from GCS to local filesystem.\n\n        The direcotry is the same on the both platforms\n\n        Inputs\n        ------\n        local_file_path : str\n            Name/Dir of the file to be downloaded with the same dir\n        '''\n        gcs_path = os.path.join(self.__bucket_dir, local_file_path)\n\n        client = storage.Client() # Use default Account/Cloud Run SA\n        bucket = client.get_bucket(self.__bucket_name)\n        blob = bucket.blob(gcs_path)\n        blob.download_to_filename(local_file_path)\n\n\n    def __debug(self, **kwargs):\n        if DEBUG:\n            print(f'\\nGoogleCloudAPI:')\n            for key, value in kwargs.items():\n                print(f'{key}:\\n{value}')\n            print('\\n')\n</code></pre>"},{"location":"reference/backend/google_cloud/api/#backend.google_cloud.api.GoogleCloudAPI.download_file_from_gcs","title":"<code>download_file_from_gcs(local_file_path)</code>","text":"<p>Download a file from GCS to local filesystem.</p> <p>The direcotry is the same on the both platforms</p> Inputs <p>local_file_path : str     Name/Dir of the file to be downloaded with the same dir</p> Source code in <code>src/backend/google_cloud/api.py</code> <pre><code>def download_file_from_gcs(self, local_file_path: str):\n    ''' Download a file from GCS to local filesystem.\n\n    The direcotry is the same on the both platforms\n\n    Inputs\n    ------\n    local_file_path : str\n        Name/Dir of the file to be downloaded with the same dir\n    '''\n    gcs_path = os.path.join(self.__bucket_dir, local_file_path)\n\n    client = storage.Client() # Use default Account/Cloud Run SA\n    bucket = client.get_bucket(self.__bucket_name)\n    blob = bucket.blob(gcs_path)\n    blob.download_to_filename(local_file_path)\n</code></pre>"},{"location":"reference/backend/google_cloud/api/#backend.google_cloud.api.GoogleCloudAPI.sql_query","title":"<code>sql_query(sql)</code>","text":"<p>Run a regular SQL query  and return a RowIterator.</p> Source code in <code>src/backend/google_cloud/api.py</code> <pre><code>def sql_query(self, sql: str) -&gt; bigquery.table.RowIterator:\n    ''' Run a regular SQL query \n    and return a RowIterator.\n    '''\n    self.__debug(sql=sql)\n    client = bigquery.Client(location=self.__location, project=self.__project_id) # Use default Account/Cloud Run SA\n    query_job = client.query(sql)\n    return query_job.result()\n</code></pre>"},{"location":"reference/backend/google_cloud/api/#backend.google_cloud.api.GoogleCloudAPI.sql_to_pandas","title":"<code>sql_to_pandas(sql)</code>","text":"<p>Run a regular SQL query  and return a pandas DataFrame.</p> Inputs <p>sql : string     A regular SQL query</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> Source code in <code>src/backend/google_cloud/api.py</code> <pre><code>def sql_to_pandas(self, sql: str) -&gt; pd.DataFrame:\n    ''' Run a regular SQL query \n    and return a pandas DataFrame.\n\n    Inputs\n    ------\n    sql : string\n        A regular SQL query\n\n    Returns\n    -------\n    df : DataFrame\n    '''\n    self.__debug(sql=sql)\n    df = pandas_gbq.read_gbq(sql, \n                             project_id=self.__project_id,\n                             location=self.__location, \n                             progress_bar_type=None) # Use default Account/Cloud Run SA\n    return df\n</code></pre>"},{"location":"reference/backend/google_cloud/api/#backend.google_cloud.api.GoogleCloudAPI.upload_file_to_gcs","title":"<code>upload_file_to_gcs(local_file_path)</code>","text":"<p>Upload Local File to GCS</p> <p>The Bucker and folder are project specific, and only the  is required Inputs <p>local_file_path : str     Name/Dir of the file to be uploaded with the same dir</p> Source code in <code>src/backend/google_cloud/api.py</code> <pre><code>def upload_file_to_gcs(self, local_file_path: str):\n    ''' Upload Local File to GCS\n\n    The Bucker and folder are project specific,\n    and only the &lt;local_file_path&gt; is required\n\n    Inputs\n    ------\n    local_file_path : str\n        Name/Dir of the file to be uploaded with the same dir\n    '''\n    gcs_path = os.path.join(self.__bucket_dir, local_file_path)\n\n    client = storage.Client() # Use default Account/Cloud Run SA\n    bucket = client.get_bucket(self.__bucket_name)\n    blob = bucket.blob(gcs_path)\n    blob.upload_from_filename(local_file_path) \n</code></pre>"},{"location":"reference/backend/google_cloud/api/#backend.google_cloud.api.GoogleCloudAPI.write_pandas_to_table","title":"<code>write_pandas_to_table(df, table)</code>","text":"<p>Push a DataFrame to BigQuery.</p> <p>A new table will be create, if the destination does not exists, however, pyarrows has a bug and it fails for datetime columns, thus the schema must be constructed manually from pandas to GBQ format. The mode is locked to Append only, to prevent accidental overwrites</p> Inputs <p>df : pd.DataFram     A regular DataFrame table : str     The name of destination Table, that is used together with initial project parameters</p> Source code in <code>src/backend/google_cloud/api.py</code> <pre><code>def write_pandas_to_table(self, df: pd.DataFrame, table: str):\n    ''' Push a DataFrame to BigQuery.\n\n    A new table will be create, if the destination does not exists,\n    however, pyarrows has a bug and it fails for datetime columns,\n    thus the schema must be constructed manually from pandas to GBQ format.\n    The mode is locked to Append only, to prevent accidental overwrites\n\n    Inputs\n    ------\n    df : pd.DataFram\n        A regular DataFrame\n    table : str\n        The name of destination Table, that is used together with initial project parameters\n    '''\n    table_schema = [] # [{'name': 'col1', 'type': 'STRING'},...]\n    for col in df.columns:\n        if 'date' in col.lower():\n             table_schema.append({'name': col, 'type': 'DATE'})\n        elif 'object' in str(df[col].dtype):\n             table_schema.append({'name': col, 'type': 'STRING'})\n        elif 'float' in str(df[col].dtype):\n            table_schema.append({'name': col, 'type': 'FLOAT64'})\n        elif 'datetime' in str(df[col].dtype):\n            table_schema.append({'name': col, 'type': 'TIMESTAMP'})\n\n    pandas_gbq.to_gbq(df, \n                      destination_table=f'{self._dataset}.{table}',\n                      project_id=self.__project_id, \n                      location=self.__location, \n                      table_schema=table_schema,\n                      if_exists='append',\n                      ) # Use default Account/Cloud Run SA\n</code></pre>"},{"location":"reference/backend/ml/ml/","title":"ml","text":""},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML","title":"<code>ML</code>","text":"Source code in <code>src/backend/ml/ml.py</code> <pre><code>class ML():\n    def __init__(self):\n        self.__client = GoogleCloudAPI()\n        self.__model = None\n        self.__model_name = os.getenv('STREAMLIT_ENV') + '_naive_bayes.pkl'\n        self.__nan = 'N/A'\n\n\n    def predict(self, data: pd.DataFrame):\n        \"\"\" Reuturns the predicted target Classes.\n\n        The model returns a dict containing all the classes,\n        in descending order, The first class is selected,\n        and the prortional probability to the total pool \n        is also returned.\n\n        Inputs\n        -----\n        data: pd.DataFrame\n            Input X Features\n\n        Returns\n        -------\n        y_predicted: list\n            Model y ouputs. If model is not loaded, fill all values by &lt;self.__nan&gt;\n\n        realative_pob: list\n            The Prob of returned classe, in relation to the total pool\n        \"\"\"\n        if self.__model is None:\n            return [self.__nan] * len(data), [0] * len(data)\n\n        preds = self.__model_get_predictions(data)\n\n        def relative_prob(pred_dict):\n            target_prob = next(iter(pred_dict.values()))\n            total_prob = sum(pred_dict.values())\n            return target_prob / total_prob\n\n        probs= [relative_prob(pred_dict) for pred_dict in preds]\n        targets = [next(iter(pred_dict.keys())) for pred_dict in preds]\n\n        return targets, probs\n\n\n    def pull_training_data(self):\n        \"\"\" Select required columns from the database\n        \"\"\"\n        sql=f\"\"\"\n        SELECT\n            KeyDate as date,\n            Receiver as receiver,\n            Amount as amount,\n            Category as category\n        FROM\n            {self.__client._dataset}.f_transactions\n        \"\"\"\n        df = self.__client.sql_to_pandas(sql)\n        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d').dt.date\n        df.sort_values('date', inplace=True)\n        df = df.dropna().reset_index(drop=True)\n        return df\n\n\n    def train_new_model(self, data:pd.DataFrame, target_col:str):\n        \"\"\" Trains and activate a new model, but does not save it automatically\n\n        The input dataframe can contain any amount of features, with different datatypes,\n        since those are automatically processed in the API.\n        The model can have any number of features, as long as string, and floats are \n        given to correct input variable.\n\n        Inputs\n        ------\n        data : pd.DataFrame\n            All columns are used to train the model\n\n        target_col : str\n            The name of the y actuall target classes\n        \"\"\"\n        data = data.loc[(data[target_col].notnull()) &amp; (data[target_col] != 'N/A')] # All rows must have a target\n        data = data.fillna('') \n        X_numeric = data.select_dtypes(include=['float']).to_numpy()\n        X_string = data.drop(target_col, axis=1).select_dtypes(include=['object']).to_numpy()\n        y = data[target_col].to_numpy()\n\n        nb = NB()\n        nb.fit(X_string, X_numeric, y)\n        self.__model = nb\n\n\n    def save_model_to_gcs(self) -&gt; bool:\n        \"\"\" Save a model to a file locally using pickle \n        and upload it to Google Cloud Storage with corresponding ENV prefix\n        \"\"\"\n        with open(self.__model_name, 'wb') as f: # Save the model locally\n            pickle.dump(self.__model, f)\n        try:\n            self.__client.upload_file_to_gcs(self.__model_name) # Initialize a GCS client and upload the file\n            return True\n        except:\n            return False\n\n    def load_model_from_gcs(self) -&gt; bool:\n        \"\"\" Download a model from GCS to the local filesystem,\n        and pickle load it.\n        \"\"\"\n        try:\n            self.__client.download_file_from_gcs(self.__model_name) # Pull the file from GCS to Local system\n\n            with open(self.__model_name,'rb') as f: # Open the local File\n                self.__model = pickle.load(f)\n            return True\n        except:\n            return False\n\n\n    def get_priors(self) -&gt; dict:\n        \"\"\" Returns the prior probabilities of all targets\n        in descending order as a dict.\n        \"\"\"\n        if self.__model is not None:\n            return self.__model.get_priors()\n        else:\n            return {}\n\n\n    def get_likelihoods(self, ntop: int = 20):\n        \"\"\" Returns the likelihoods for all possible feature token\n        and ascending order.\n        The Dict is format likes[&lt;target_name&gt;][&lt;toke&gt;]:value\n        All levels are in descending order, and all targets contain \n        all possible tokens\n\n        Inputs\n        -----\n        ntop : int\n            Select only the top n tokens\n        \"\"\"\n        if self.__model is not None:\n            likes = self.__model.get_likelihoods()\n            return {key: dict(list(sub_dict.items())[:ntop]) for key, sub_dict in likes.items()}\n        else:\n            return {}\n\n\n    def validate_model(self, data:pd.DataFrame, target_col:str, accepted_error=1):\n        \"\"\" Returns the Accuracy information for the given model.\n\n        Inputs\n        ------\n        data : pd.DataFram\n            Validation dataframe\n\n        target_col : str\n            Name of the target column\n\n        accepted_error : int\n            The maximum allowed deviation from the first place of the target list.\n            The model return a list of all available classes in the order of \n            the propability, and some deviation is allowed when computing the \n            overall accuracy (usefulnes) of the model\n        \"\"\"\n        y_predicted = self.__model_get_predictions(data.drop(target_col, axis=1))\n        wa, stats = self.__get_statistics(y_predicted, y_valid=data[target_col].to_numpy(), accepted_error=accepted_error)\n        return wa, stats\n\n\n    def has_model(self):\n        \"\"\" Used to check if the model has been initialized\n        \"\"\"\n        return self.__model is not None\n\n\n    def __model_get_predictions(self, data: pd.DataFrame)-&gt;dict:\n        \"\"\" Run model prediction\n\n        Returns\n        -------\n        predictions : dict\n            A dictionary containing all target classe in descending order of the propability\n        \"\"\"\n        data = data.fillna('') \n        data = data.drop(columns=[col for col in data.columns if any(isinstance(val, (datetime.date, datetime.datetime)) for val in data[col])]) # Remove the Date-type column\n        X_numeric = data.select_dtypes(include=['float']).to_numpy()\n        X_string = data.select_dtypes(include=['object']).to_numpy()\n        preds = self.__model.predict(X_string, X_numeric)\n        return preds\n\n\n    def __get_statistics(self, y_predicted: list, y_valid: list, accepted_error: int):\n        \"\"\" A helper function to compute accuracy statistics for the trained model.\n\n        Inputs\n        ------\n        y_predicted : list\n            Model ouputs for the predicted target classes\n\n        y_valid: list\n            Actual target classes\n\n        accepted_error : int\n            The maximum allowed deviation from the first place of the target list.\n            The model return a list of all available classes in the order of \n            the propability, and some deviation is allowed when computing the \n            overall accuracy (usefulnes) of the model\n        \"\"\"\n        def get_accuracy(series):\n            count = series.shape[0]\n            if count == 0:\n                return 0.0\n            return float(series.sum()) / count\n\n        errors = []\n        labels = []\n        for i, row in enumerate(y_predicted):\n            try:\n                errors.append(int(list(row.keys()).index(y_valid[i])))\n                labels.append(y_valid[i])\n            except ValueError:\n                pass\n        df_errors = pd.DataFrame(zip(labels, errors), columns=['y_valid', 'order'])\n\n        df_accuracy = df_errors.copy()\n        df_accuracy['acceptable'] = df_accuracy['order'] &lt;= accepted_error\n        df_accuracy = df_accuracy.groupby(['y_valid'])['acceptable'].apply(lambda x: get_accuracy(x)).rename('accuracy').reset_index()\n\n        df_stats = df_errors.groupby('y_valid').agg(count=('order', 'count'),\n                                                    place_q50=('order', lambda x: np.percentile(x, 50)),\n                                                    ).reset_index()\n\n        df_stats = pd.merge(df_stats, df_accuracy, left_on='y_valid', right_on='y_valid')\n        df_stats = df_stats.sort_values(by='count', ascending=False).reset_index(drop=True)\n\n        w_accuracy = np.average(df_stats['accuracy'].values, weights=df_stats['count'].values) # Weighted accuracy\n        return w_accuracy, df_stats\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.__get_statistics","title":"<code>__get_statistics(y_predicted, y_valid, accepted_error)</code>","text":"<p>A helper function to compute accuracy statistics for the trained model.</p> Inputs <p>y_predicted : list     Model ouputs for the predicted target classes</p> <p>y_valid: list     Actual target classes</p> <p>accepted_error : int     The maximum allowed deviation from the first place of the target list.     The model return a list of all available classes in the order of      the propability, and some deviation is allowed when computing the      overall accuracy (usefulnes) of the model</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def __get_statistics(self, y_predicted: list, y_valid: list, accepted_error: int):\n    \"\"\" A helper function to compute accuracy statistics for the trained model.\n\n    Inputs\n    ------\n    y_predicted : list\n        Model ouputs for the predicted target classes\n\n    y_valid: list\n        Actual target classes\n\n    accepted_error : int\n        The maximum allowed deviation from the first place of the target list.\n        The model return a list of all available classes in the order of \n        the propability, and some deviation is allowed when computing the \n        overall accuracy (usefulnes) of the model\n    \"\"\"\n    def get_accuracy(series):\n        count = series.shape[0]\n        if count == 0:\n            return 0.0\n        return float(series.sum()) / count\n\n    errors = []\n    labels = []\n    for i, row in enumerate(y_predicted):\n        try:\n            errors.append(int(list(row.keys()).index(y_valid[i])))\n            labels.append(y_valid[i])\n        except ValueError:\n            pass\n    df_errors = pd.DataFrame(zip(labels, errors), columns=['y_valid', 'order'])\n\n    df_accuracy = df_errors.copy()\n    df_accuracy['acceptable'] = df_accuracy['order'] &lt;= accepted_error\n    df_accuracy = df_accuracy.groupby(['y_valid'])['acceptable'].apply(lambda x: get_accuracy(x)).rename('accuracy').reset_index()\n\n    df_stats = df_errors.groupby('y_valid').agg(count=('order', 'count'),\n                                                place_q50=('order', lambda x: np.percentile(x, 50)),\n                                                ).reset_index()\n\n    df_stats = pd.merge(df_stats, df_accuracy, left_on='y_valid', right_on='y_valid')\n    df_stats = df_stats.sort_values(by='count', ascending=False).reset_index(drop=True)\n\n    w_accuracy = np.average(df_stats['accuracy'].values, weights=df_stats['count'].values) # Weighted accuracy\n    return w_accuracy, df_stats\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.__model_get_predictions","title":"<code>__model_get_predictions(data)</code>","text":"<p>Run model prediction</p> <p>Returns:</p> Name Type Description <code>predictions</code> <code>dict</code> <p>A dictionary containing all target classe in descending order of the propability</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def __model_get_predictions(self, data: pd.DataFrame)-&gt;dict:\n    \"\"\" Run model prediction\n\n    Returns\n    -------\n    predictions : dict\n        A dictionary containing all target classe in descending order of the propability\n    \"\"\"\n    data = data.fillna('') \n    data = data.drop(columns=[col for col in data.columns if any(isinstance(val, (datetime.date, datetime.datetime)) for val in data[col])]) # Remove the Date-type column\n    X_numeric = data.select_dtypes(include=['float']).to_numpy()\n    X_string = data.select_dtypes(include=['object']).to_numpy()\n    preds = self.__model.predict(X_string, X_numeric)\n    return preds\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.get_likelihoods","title":"<code>get_likelihoods(ntop=20)</code>","text":"<p>Returns the likelihoods for all possible feature token and ascending order. The Dict is format likes[][]:value All levels are in descending order, and all targets contain  all possible tokens Inputs <p>ntop : int     Select only the top n tokens</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def get_likelihoods(self, ntop: int = 20):\n    \"\"\" Returns the likelihoods for all possible feature token\n    and ascending order.\n    The Dict is format likes[&lt;target_name&gt;][&lt;toke&gt;]:value\n    All levels are in descending order, and all targets contain \n    all possible tokens\n\n    Inputs\n    -----\n    ntop : int\n        Select only the top n tokens\n    \"\"\"\n    if self.__model is not None:\n        likes = self.__model.get_likelihoods()\n        return {key: dict(list(sub_dict.items())[:ntop]) for key, sub_dict in likes.items()}\n    else:\n        return {}\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.get_priors","title":"<code>get_priors()</code>","text":"<p>Returns the prior probabilities of all targets in descending order as a dict.</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def get_priors(self) -&gt; dict:\n    \"\"\" Returns the prior probabilities of all targets\n    in descending order as a dict.\n    \"\"\"\n    if self.__model is not None:\n        return self.__model.get_priors()\n    else:\n        return {}\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.has_model","title":"<code>has_model()</code>","text":"<p>Used to check if the model has been initialized</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def has_model(self):\n    \"\"\" Used to check if the model has been initialized\n    \"\"\"\n    return self.__model is not None\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.load_model_from_gcs","title":"<code>load_model_from_gcs()</code>","text":"<p>Download a model from GCS to the local filesystem, and pickle load it.</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def load_model_from_gcs(self) -&gt; bool:\n    \"\"\" Download a model from GCS to the local filesystem,\n    and pickle load it.\n    \"\"\"\n    try:\n        self.__client.download_file_from_gcs(self.__model_name) # Pull the file from GCS to Local system\n\n        with open(self.__model_name,'rb') as f: # Open the local File\n            self.__model = pickle.load(f)\n        return True\n    except:\n        return False\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.predict","title":"<code>predict(data)</code>","text":"<p>Reuturns the predicted target Classes.</p> <p>The model returns a dict containing all the classes, in descending order, The first class is selected, and the prortional probability to the total pool  is also returned.</p> Inputs <p>data: pd.DataFrame     Input X Features</p> <p>Returns:</p> Name Type Description <code>y_predicted</code> <code>list</code> <p>Model y ouputs. If model is not loaded, fill all values by  <code>realative_pob</code> <code>list</code> <p>The Prob of returned classe, in relation to the total pool</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def predict(self, data: pd.DataFrame):\n    \"\"\" Reuturns the predicted target Classes.\n\n    The model returns a dict containing all the classes,\n    in descending order, The first class is selected,\n    and the prortional probability to the total pool \n    is also returned.\n\n    Inputs\n    -----\n    data: pd.DataFrame\n        Input X Features\n\n    Returns\n    -------\n    y_predicted: list\n        Model y ouputs. If model is not loaded, fill all values by &lt;self.__nan&gt;\n\n    realative_pob: list\n        The Prob of returned classe, in relation to the total pool\n    \"\"\"\n    if self.__model is None:\n        return [self.__nan] * len(data), [0] * len(data)\n\n    preds = self.__model_get_predictions(data)\n\n    def relative_prob(pred_dict):\n        target_prob = next(iter(pred_dict.values()))\n        total_prob = sum(pred_dict.values())\n        return target_prob / total_prob\n\n    probs= [relative_prob(pred_dict) for pred_dict in preds]\n    targets = [next(iter(pred_dict.keys())) for pred_dict in preds]\n\n    return targets, probs\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.pull_training_data","title":"<code>pull_training_data()</code>","text":"<p>Select required columns from the database</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def pull_training_data(self):\n    \"\"\" Select required columns from the database\n    \"\"\"\n    sql=f\"\"\"\n    SELECT\n        KeyDate as date,\n        Receiver as receiver,\n        Amount as amount,\n        Category as category\n    FROM\n        {self.__client._dataset}.f_transactions\n    \"\"\"\n    df = self.__client.sql_to_pandas(sql)\n    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d').dt.date\n    df.sort_values('date', inplace=True)\n    df = df.dropna().reset_index(drop=True)\n    return df\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.save_model_to_gcs","title":"<code>save_model_to_gcs()</code>","text":"<p>Save a model to a file locally using pickle  and upload it to Google Cloud Storage with corresponding ENV prefix</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def save_model_to_gcs(self) -&gt; bool:\n    \"\"\" Save a model to a file locally using pickle \n    and upload it to Google Cloud Storage with corresponding ENV prefix\n    \"\"\"\n    with open(self.__model_name, 'wb') as f: # Save the model locally\n        pickle.dump(self.__model, f)\n    try:\n        self.__client.upload_file_to_gcs(self.__model_name) # Initialize a GCS client and upload the file\n        return True\n    except:\n        return False\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.train_new_model","title":"<code>train_new_model(data, target_col)</code>","text":"<p>Trains and activate a new model, but does not save it automatically</p> <p>The input dataframe can contain any amount of features, with different datatypes, since those are automatically processed in the API. The model can have any number of features, as long as string, and floats are  given to correct input variable.</p> Inputs <p>data : pd.DataFrame     All columns are used to train the model</p> <p>target_col : str     The name of the y actuall target classes</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def train_new_model(self, data:pd.DataFrame, target_col:str):\n    \"\"\" Trains and activate a new model, but does not save it automatically\n\n    The input dataframe can contain any amount of features, with different datatypes,\n    since those are automatically processed in the API.\n    The model can have any number of features, as long as string, and floats are \n    given to correct input variable.\n\n    Inputs\n    ------\n    data : pd.DataFrame\n        All columns are used to train the model\n\n    target_col : str\n        The name of the y actuall target classes\n    \"\"\"\n    data = data.loc[(data[target_col].notnull()) &amp; (data[target_col] != 'N/A')] # All rows must have a target\n    data = data.fillna('') \n    X_numeric = data.select_dtypes(include=['float']).to_numpy()\n    X_string = data.drop(target_col, axis=1).select_dtypes(include=['object']).to_numpy()\n    y = data[target_col].to_numpy()\n\n    nb = NB()\n    nb.fit(X_string, X_numeric, y)\n    self.__model = nb\n</code></pre>"},{"location":"reference/backend/ml/ml/#backend.ml.ml.ML.validate_model","title":"<code>validate_model(data, target_col, accepted_error=1)</code>","text":"<p>Returns the Accuracy information for the given model.</p> Inputs <p>data : pd.DataFram     Validation dataframe</p> <p>target_col : str     Name of the target column</p> <p>accepted_error : int     The maximum allowed deviation from the first place of the target list.     The model return a list of all available classes in the order of      the propability, and some deviation is allowed when computing the      overall accuracy (usefulnes) of the model</p> Source code in <code>src/backend/ml/ml.py</code> <pre><code>def validate_model(self, data:pd.DataFrame, target_col:str, accepted_error=1):\n    \"\"\" Returns the Accuracy information for the given model.\n\n    Inputs\n    ------\n    data : pd.DataFram\n        Validation dataframe\n\n    target_col : str\n        Name of the target column\n\n    accepted_error : int\n        The maximum allowed deviation from the first place of the target list.\n        The model return a list of all available classes in the order of \n        the propability, and some deviation is allowed when computing the \n        overall accuracy (usefulnes) of the model\n    \"\"\"\n    y_predicted = self.__model_get_predictions(data.drop(target_col, axis=1))\n    wa, stats = self.__get_statistics(y_predicted, y_valid=data[target_col].to_numpy(), accepted_error=accepted_error)\n    return wa, stats\n</code></pre>"},{"location":"reference/backend/ml/model/","title":"model","text":""}]}